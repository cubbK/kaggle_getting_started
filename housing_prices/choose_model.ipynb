{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5ed2b0",
   "metadata": {},
   "source": [
    "Let's clean our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "192107f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('SalePrice', axis=1)  # assuming 'SalePrice' is your target\n",
    "y = df['SalePrice']\n",
    "\n",
    "def encode_categorical_features(X):\n",
    "    \"\"\"\n",
    "    Encodes categorical features in the DataFrame X using OneHotEncoder.\n",
    "    Returns a DataFrame with encoded features.\n",
    "    \"\"\"\n",
    "    # Get categorical columns from features only\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Handle missing values first\n",
    "    X_clean = X.copy()\n",
    "    # for col in categorical_cols:\n",
    "    #     X_clean[col] = X_clean[col].fillna('Missing')\n",
    "\n",
    "    # Fill numeric missing values\n",
    "    numeric_cols = X_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        X_clean[col] = X_clean[col].fillna(X_clean[col].median())\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "        ],\n",
    "        remainder='passthrough'  # keep other (non-categorical) columns\n",
    "    )\n",
    "\n",
    "    transformed = preprocessor.fit_transform(X_clean)\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "    return pd.DataFrame(transformed, columns=feature_names) # type: ignore\n",
    "\n",
    "final_df = encode_categorical_features(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f552bd2",
   "metadata": {},
   "source": [
    "Let's use a baseline model first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3274cea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (CV): $35,433.85 (+/- $16,530.80)\n",
      "Percentage Error (CV): 19.59% (+/- 9.14%)\n",
      "Target variable mean: $180,921.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "# Use all your training data for CV\n",
    "cv_scores = cross_val_score(lr_model, final_df, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "def print_scores(scores):\n",
    "    rmse = np.sqrt(-scores)\n",
    "    # Calculate percentage error\n",
    "    mean_target = y.mean()\n",
    "    rmse_mean = rmse.mean()\n",
    "    rmse_std = rmse.std()\n",
    "\n",
    "    # Calculate percentage error\n",
    "    percentage_error = (rmse_mean / mean_target) * 100\n",
    "    percentage_error_std = (rmse_std * 2 / mean_target) * 100\n",
    "\n",
    "    print(f\"Model Performance (CV): ${rmse_mean:,.2f} (+/- ${rmse_std * 2:,.2f})\")\n",
    "    print(f\"Percentage Error (CV): {percentage_error:.2f}% (+/- {percentage_error_std:.2f}%)\")\n",
    "    print(f\"Target variable mean: ${mean_target:,.2f}\")\n",
    "    \n",
    "print_scores(cv_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d33f5c",
   "metadata": {},
   "source": [
    "Now that we have a baseline let's try more models and choose one that performs best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "edb87397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (CV): $30,095.84 (+/- $7,696.55)\n",
      "Percentage Error (CV): 16.63% (+/- 4.25%)\n",
      "Target variable mean: $180,921.20\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_cv_scores = cross_val_score(rf_model, final_df, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print_scores(rf_cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ad077ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (CV): $27,268.95 (+/- $8,965.59)\n",
      "Percentage Error (CV): 15.07% (+/- 4.96%)\n",
      "Target variable mean: $180,921.20\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "\n",
    "xgb_cv_scores = cross_val_score(xgb_model, final_df, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print_scores(xgb_cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "21f35ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (CV): $26,219.22 (+/- $6,816.72)\n",
      "Percentage Error (CV): 14.49% (+/- 3.77%)\n",
      "Target variable mean: $180,921.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "\n",
    "gb_cv_scores = cross_val_score(gb_model, final_df, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print_scores(gb_cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "78b4740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (CV): $28,685.10 (+/- $8,337.02)\n",
      "Percentage Error (CV): 15.86% (+/- 4.61%)\n",
      "Target variable mean: $180,921.20\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, random_state=42, learning_rate=0.1, verbose=-1)\n",
    "\n",
    "lgb_cv_scores = cross_val_score(lgb_model, final_df, y, cv=5, scoring='neg_mean_squared_error') # type: ignore\n",
    "\n",
    "print_scores(lgb_cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c0a415f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (CV): $79,331.77 (+/- $12,720.58)\n",
      "Percentage Error (CV): 43.85% (+/- 7.03%)\n",
      "Target variable mean: $180,921.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# SVR works better with scaled features\n",
    "svr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR(kernel='rbf', C=100, gamma='scale'))\n",
    "])\n",
    "\n",
    "svr_cv_scores = cross_val_score(svr_pipeline, final_df, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print_scores(svr_cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d340a1",
   "metadata": {},
   "source": [
    "From all our models GradientBoosting gives the best results. Let's do feature engineering now to improve our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "91735498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (CV): $26,702.86 (+/- $7,965.73)\n",
      "Percentage Error (CV): 14.76% (+/- 4.40%)\n",
      "Target variable mean: $180,921.20\n"
     ]
    }
   ],
   "source": [
    "# Let's remove Id\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X_clean = X.copy()\n",
    "X_clean = X_clean.drop(columns=[\"Id\"]) # remove numerical features with 0.03 < correlation > -0.03. Removing more gives worse results\n",
    "\n",
    "final_df = encode_categorical_features(X_clean)\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "\n",
    "gb_cv_scores = cross_val_score(gb_model, final_df, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print_scores(gb_cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9726029f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (CV): $26,289.06 (+/- $7,933.59)\n",
      "Percentage Error (CV): 14.53% (+/- 4.39%)\n",
      "Target variable mean: $180,921.20\n"
     ]
    }
   ],
   "source": [
    "# Let's remove features with very weak correlation (close to zero)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X_clean = X.copy()\n",
    "X_clean = X_clean.drop(columns=['BsmtFinSF2', 'BsmtHalfBath', 'MiscVal', \"Id\"]) # remove numerical features with 0.03 < correlation > -0.03. Removing more gives worse results\n",
    "\n",
    "final_df = encode_categorical_features(X_clean)\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "\n",
    "gb_cv_scores = cross_val_score(gb_model, final_df, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print_scores(gb_cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "13beb76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (CV): $26,589.31 (+/- $6,461.66)\n",
      "Percentage Error (CV): 14.70% (+/- 3.57%)\n",
      "Target variable mean: $180,921.20\n"
     ]
    }
   ],
   "source": [
    "# Let's remove features with very weak correlation (close to zero)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X_clean = X.copy()\n",
    "X_clean = X_clean.drop(columns=['BsmtFinSF2', 'BsmtHalfBath', 'MiscVal', \"Id\", 'LowQualFinSF', 'YrSold']) # remove numerical features with 0.03 < correlation > -0.03. Removing more gives worse results\n",
    "\n",
    "final_df = encode_categorical_features(X_clean)\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "\n",
    "gb_cv_scores = cross_val_score(gb_model, final_df, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print_scores(gb_cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d69eb3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Method 4: Tree-based Feature Importance ===\n",
      "Categorical feature importances:\n",
      "BsmtQual: 0.0054\n",
      "Neighborhood: 0.0049\n",
      "GarageType: 0.0039\n",
      "KitchenQual: 0.0039\n",
      "ExterQual: 0.0034\n",
      "BsmtExposure: 0.0025\n",
      "Exterior1st: 0.0023\n",
      "GarageFinish: 0.0022\n",
      "CentralAir: 0.0022\n",
      "FireplaceQu: 0.0021\n",
      "Exterior2nd: 0.0021\n",
      "MSZoning: 0.0019\n",
      "LandContour: 0.0016\n",
      "MasVnrType: 0.0016\n",
      "LotShape: 0.0015\n",
      "BsmtFinType1: 0.0013\n",
      "SaleCondition: 0.0012\n",
      "SaleType: 0.0012\n",
      "LotConfig: 0.0011\n",
      "Condition1: 0.0011\n",
      "HouseStyle: 0.0010\n",
      "RoofStyle: 0.0010\n",
      "GarageQual: 0.0009\n",
      "Heating: 0.0008\n",
      "Foundation: 0.0008\n",
      "ExterCond: 0.0008\n",
      "BsmtFinType2: 0.0007\n",
      "HeatingQC: 0.0007\n",
      "GarageCond: 0.0007\n",
      "Functional: 0.0005\n",
      "BldgType: 0.0005\n",
      "Fence: 0.0005\n",
      "RoofMatl: 0.0005\n",
      "LandSlope: 0.0004\n",
      "BsmtCond: 0.0004\n",
      "PavedDrive: 0.0004\n",
      "Alley: 0.0003\n",
      "Condition2: 0.0002\n",
      "Electrical: 0.0002\n",
      "PoolQC: 0.0001\n",
      "MiscFeature: 0.0001\n",
      "Street: 0.0000\n",
      "Utilities: 0.0000\n",
      "\n",
      "Low importance features: ['Street', 'Alley', 'Utilities', 'LandSlope', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterCond', 'Foundation', 'BsmtCond', 'BsmtFinType2', 'Heating', 'HeatingQC', 'Electrical', 'Functional', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature']\n",
      "Model Performance (CV): $26,463.59 (+/- $6,847.97)\n",
      "Percentage Error (CV): 14.63% (+/- 3.79%)\n",
      "Target variable mean: $180,921.20\n"
     ]
    }
   ],
   "source": [
    "# Let's try to remove weak categorical features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Method 4: Tree-based Feature Importance\n",
    "def get_low_importance_categorical(X, y, importance_threshold=0.001):\n",
    "    \"\"\"\n",
    "    Use Random Forest to identify categorical features with low importance\n",
    "    \"\"\"\n",
    "    # Encode categorical features temporarily\n",
    "    temp_df = encode_categorical_features(X)\n",
    "    \n",
    "    # Fit Random Forest to get feature importances\n",
    "    rf_temp = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    rf_temp.fit(temp_df, y)\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': temp_df.columns,\n",
    "        'importance': rf_temp.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Identify original categorical column names from encoded features\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_importance = {}\n",
    "    \n",
    "    for cat_col in categorical_cols:\n",
    "        # Find all encoded features that belong to this categorical column\n",
    "        encoded_features = [f for f in temp_df.columns if f.startswith(f'cat__{cat_col}')]\n",
    "        # Sum importances of all encoded features for this categorical column\n",
    "        total_importance = feature_importance[feature_importance['feature'].isin(encoded_features)]['importance'].sum()\n",
    "        categorical_importance[cat_col] = total_importance\n",
    "    \n",
    "    # Identify low importance categorical features\n",
    "    low_importance_features = [col for col, imp in categorical_importance.items() \n",
    "                              if imp < importance_threshold]\n",
    "    \n",
    "    return low_importance_features, categorical_importance\n",
    "\n",
    "# Apply Method 4: Tree-based Feature Importance\n",
    "print(\"\\n=== Method 4: Tree-based Feature Importance ===\")\n",
    "low_importance_features, cat_importance = get_low_importance_categorical(X_clean, y)\n",
    "\n",
    "print(\"Categorical feature importances:\")\n",
    "sorted_importance = sorted(cat_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "print(f\"\\nLow importance features: {low_importance_features}\")\n",
    "\n",
    "\n",
    "X_clean_clean = X.copy()\n",
    "X_clean_clean.drop(columns=low_importance_features, inplace=True)\n",
    "\n",
    "final_df_clean = encode_categorical_features(X_clean_clean)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "gb_cv_scores_clean = cross_val_score(gb_model, final_df_clean, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print_scores(gb_cv_scores_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3edb6",
   "metadata": {},
   "source": [
    "Removing these features worsen our score. Let's remove only the limited ['BsmtFinSF2', 'BsmtHalfBath', 'MiscVal', \"Id\", 'LowQualFinSF', 'YrSold'] that give us a base score of 14.70% (+/- 3.57%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "75193799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (CV): $25,893.63 (+/- $6,970.47)\n",
      "Percentage Error (CV): 14.31% (+/- 3.85%)\n",
      "Target variable mean: $180,921.20\n"
     ]
    }
   ],
   "source": [
    "# Let's feature engineer new columns\n",
    "\n",
    "X_new_features = X.copy()\n",
    "\n",
    "# Aggregate Features\n",
    "\n",
    "# Total bathrooms\n",
    "X_new_features['TotalBath'] = X_new_features['FullBath'] + X_new_features['HalfBath'] * 0.5 + X_new_features['BsmtFullBath'] + X_new_features['BsmtHalfBath'] * 0.5\n",
    "\n",
    "# Total porch area\n",
    "X_new_features['TotalPorchSF'] = X_new_features['OpenPorchSF'] + X_new_features['EnclosedPorch'] + X_new_features['3SsnPorch'] + X_new_features['ScreenPorch']\n",
    "\n",
    "# Living area per room\n",
    "X_new_features['LivAreaPerRoom'] = X_new_features['GrLivArea'] / X_new_features['TotRmsAbvGrd']\n",
    "\n",
    "# Age-Based Features\n",
    "\n",
    "# House age at time of sale\n",
    "X_new_features['HouseAge'] = X_new_features['YrSold'] - X_new_features['YearBuilt']\n",
    "\n",
    "# Years since remodel\n",
    "X_new_features['YearsSinceRemodel'] = X_new_features['YrSold'] - X_new_features['YearRemodAdd']\n",
    "\n",
    "# Garage age\n",
    "X_new_features['GarageAge'] = X_new_features['YrSold'] - X_new_features['GarageYrBlt']\n",
    "\n",
    "# Was house remodeled?\n",
    "X_new_features['IsRemodeled'] = (X_new_features['YearRemodAdd'] != X_new_features['YearBuilt']).astype(int)\n",
    "\n",
    "\n",
    "# Quality/Condition Combinations\n",
    "\n",
    "# Overall quality * condition interaction\n",
    "X_new_features['QualCondProduct'] = X_new_features['OverallQual'] * X_new_features['OverallCond']\n",
    "\n",
    "# Quality scores (convert categorical to numerical)\n",
    "quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1}\n",
    "X_new_features['ExterQual_num'] = X_new_features['ExterQual'].map(quality_map)\n",
    "X_new_features['KitchenQual_num'] = X_new_features['KitchenQual'].map(quality_map)\n",
    "\n",
    "# Average quality score\n",
    "X_new_features['AvgQuality'] = (X_new_features['OverallQual'] + X_new_features['ExterQual_num'] + X_new_features['KitchenQual_num']) / 3\n",
    "\n",
    "\n",
    "# Has specific features\n",
    "X_new_features['HasPool'] = (X_new_features['PoolArea'] > 0).astype(int)\n",
    "X_new_features['HasGarage'] = (X_new_features['GarageArea'] > 0).astype(int)\n",
    "X_new_features['HasBasement'] = (X_new_features['TotalBsmtSF'] > 0).astype(int)\n",
    "X_new_features['HasFireplace'] = (X_new_features['Fireplaces'] > 0).astype(int)\n",
    "X_new_features['HasWoodDeck'] = (X_new_features['WoodDeckSF'] > 0).astype(int)\n",
    "X_new_features['HasFence'] = (X_new_features['Fence'] != 'NA').astype(int)\n",
    "\n",
    "\n",
    "# Garage to lot area ratio\n",
    "X_new_features['GarageRatio'] = X_new_features['GarageArea'] / X_new_features['LotArea']\n",
    "\n",
    "# Living area to lot area ratio\n",
    "X_new_features['LivAreaRatio'] = X_new_features['GrLivArea'] / X_new_features['LotArea']\n",
    "\n",
    "\n",
    "# Is corner lot\n",
    "X_new_features['IsCornerLot'] = (X_new_features['LotConfig'] == 'Corner').astype(int)\n",
    "\n",
    "# Sale season\n",
    "X_new_features['SaleSeason'] = X_new_features['MoSold'].apply(lambda x: 'Spring' if x in [3,4,5] \n",
    "                                      else 'Summer' if x in [6,7,8]\n",
    "                                      else 'Fall' if x in [9,10,11] \n",
    "                                      else 'Winter')\n",
    "\n",
    "# House size categories\n",
    "X_new_features['HouseSizeCategory'] = pd.cut(X_new_features['GrLivArea'], \n",
    "                                bins=[0, 1200, 1800, 2500, float('inf')], \n",
    "                                labels=['Small', 'Medium', 'Large', 'XLarge'])\n",
    "\n",
    "\n",
    "# Create indicators for missing values in key features\n",
    "missing_features = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n",
    "for feature in missing_features:\n",
    "    X_new_features[f'{feature}_Missing'] = X_new_features[feature].isnull().astype(int)\n",
    "    \n",
    "final_df = encode_categorical_features(X_new_features)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "gb_cv_scores_clean = cross_val_score(gb_model, final_df, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print_scores(gb_cv_scores_clean)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
